#!/usr/bin/env python
# coding: utf-8

# # **ğŸ  ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ Baseline code**
# > ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ì˜ˆì¸¡ ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰     
# > ì•„ë˜ baselineì—ì„œëŠ” RandomForestë¥¼ í™œìš©í•´ ML ë°©ë²•ë¡ ë“¤ì„ ì‹¤ì œ ëŒ€íšŒì— ì ìš©í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
# > ê°•ì˜ëŠ” google colabìœ¼ë¡œ ì‹¤í–‰í•˜ì˜€ê¸°ì— ì•„ë˜ì˜ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œì™€ëŠ” ì¼ë¶€ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
# 
# ## Contents
# - Library Import
# - Data Load
# - Data Preprocessing
# - Feature Engineering
# - Model Training
# - Inference
# - Output File Save
# 

# ## 1. Library Import
# - í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

# In[ ]:
from IPython.display import display

# get_ipython().system('pip install eli5==0.13.0')

# # í•œê¸€ í°íŠ¸ ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
# get_ipython().system('apt-get install -y fonts-nanum')


# In[ ]:


# visualization
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
fe = fm.FontEntry(
    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf íŒŒì¼ì´ ì €ì¥ë˜ì–´ ìˆëŠ” ê²½ë¡œ
    name='NanumBarunGothic')                        # ì´ í°íŠ¸ì˜ ì›í•˜ëŠ” ì´ë¦„ ì„¤ì •
fm.fontManager.ttflist.insert(0, fe)              # Matplotlibì— í°íŠ¸ ì¶”ê°€
plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # í°íŠ¸ ì„¤ì •
plt.rc('font', family='NanumBarunGothic')
import seaborn as sns

# utils
import pandas as pd
import numpy as np
from tqdm import tqdm
import pickle
import warnings;warnings.filterwarnings('ignore')

# Model
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics

import eli5
from eli5.sklearn import PermutationImportance


# ## 2. Data Load

# #### 2.1. ë°ì´í„° ë¡œë“œ

# In[ ]:


# í•„ìš”í•œ ë°ì´í„°ë¥¼ load í•˜ê² ìŠµë‹ˆë‹¤. ê²½ë¡œëŠ” í™˜ê²½ì— ë§ê²Œ ì§€ì •í•´ì£¼ë©´ ë©ë‹ˆë‹¤.
train_path = '/data/ephemeral/home/train.csv' 
test_path  = '/data/ephemeral/home/AI_Portfolio/AI_Projects/House_Price_Prediction/competition/data/test.csv'
dt = pd.read_csv(train_path)
dt_test = pd.read_csv(test_path)


# In[ ]:


# pd.set_option('display.max_columns', None)
dt.info()
# print(dt)


# In[ ]:


# Train dataì™€ Test data shapeì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
print('Train data shape : ', dt.shape, 'Test data shape : ', dt_test.shape)


# In[ ]:


# Trainê³¼ Test dataë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
display(dt.head(1))
display(dt_test.head(1))      # ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€(=Target) columnì´ ì œì™¸ëœ ëª¨ìŠµì…ë‹ˆë‹¤.


# In[ ]:


dt.columns


# ## 3. Data Preprocessing

# - ëª¨ë¸ë§ ì „ì— ë°ì´í„° ë‚´ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ë“±ì„ ì œê±°í•˜ê³  ë²”ì£¼í˜•ê³¼ ì—°ì†í˜• ë³€ìˆ˜ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤!
# - ë¨¼ì €, ìš©ì´í•œ ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ trainê³¼ test dataë¥¼ í•©ì¹œ í•˜ë‚˜ì˜ ë°ì´í„°ë¡œ ì§„í–‰í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

# In[ ]:


# train/test êµ¬ë¶„ì„ ìœ„í•œ ì¹¼ëŸ¼ì„ í•˜ë‚˜ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.
dt['is_test'] = 0
dt_test['is_test'] = 1
concat = pd.concat([dt, dt_test])     # í•˜ë‚˜ì˜ ë°ì´í„°ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
print(concat)

concat['is_test'].value_counts()      # trainê³¼ test dataê°€ í•˜ë‚˜ë¡œ í•©ì³ì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ì¹¼ëŸ¼ ì´ë¦„ì„ ì‰½ê²Œ ë°”ê¿”ì£¼ê² ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¹¼ëŸ¼ë„ ì‚¬ìš©ì— ë”°ë¼ ë°”ê¿”ì£¼ì…”ë„ ë©ë‹ˆë‹¤!
concat = concat.rename(columns={'ì „ìš©ë©´ì (ã¡)':'ì „ìš©ë©´ì '})
print(concat)

# ì‹¤ì œë¡œ ê²°ì¸¡ì¹˜ë¼ê³  í‘œì‹œëŠ” ì•ˆë˜ì–´ìˆì§€ë§Œ ì•„ë¬´ ì˜ë¯¸ë„ ê°–ì§€ ì•ŠëŠ” elementë“¤ì´ ì•„ë˜ì™€ ê°™ì´ ì¡´ì¬í•©ë‹ˆë‹¤.
# ì•„ë˜ 3ê°€ì§€ì˜ ê²½ìš° ëª¨ë‘ ì•„ë¬´ ì˜ë¯¸ë„ ê°–ì§€ ì•ŠëŠ” elementê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
display(concat['ë“±ê¸°ì‹ ì²­ì¼ì'].value_counts())


# In[ ]:


display(concat['ê±°ë˜ìœ í˜•'].value_counts())


# In[ ]:


display(concat['ì¤‘ê°œì‚¬ì†Œì¬ì§€'].value_counts())


# In[ ]:


# ìœ„ ì²˜ëŸ¼ ì•„ë¬´ ì˜ë¯¸ë„ ê°–ì§€ ì•ŠëŠ” ì¹¼ëŸ¼ì€ ê²°ì¸¡ì¹˜ì™€ ê°™ì€ ì—­í• ì„ í•˜ë¯€ë¡œ, np.nanìœ¼ë¡œ ì±„ì›Œ ê²°ì¸¡ì¹˜ë¡œ ì¸ì‹ë˜ë„ë¡ í•©ë‹ˆë‹¤.
concat['ë“±ê¸°ì‹ ì²­ì¼ì'] = concat['ë“±ê¸°ì‹ ì²­ì¼ì'].replace(' ', np.nan)
concat['ê±°ë˜ìœ í˜•'] = concat['ê±°ë˜ìœ í˜•'].replace('-', np.nan)
concat['ì¤‘ê°œì‚¬ì†Œì¬ì§€'] = concat['ì¤‘ê°œì‚¬ì†Œì¬ì§€'].replace('-', np.nan)


# In[ ]:


# EDAì— ì•ì„œ ê²°ì¸¡ì¹˜ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.
concat.isnull().sum()


# In[ ]:


# ë³€ìˆ˜ë³„ ê²°ì¸¡ì¹˜ì˜ ë¹„ìœ¨ì„ plotìœ¼ë¡œ ê·¸ë ¤ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
fig = plt.figure(figsize=(13, 2))
missing = concat.isnull().sum() / concat.shape[0]
missing = missing[missing > 0]
missing.sort_values(inplace=True)
missing.plot.bar(color='orange')
plt.title('ë³€ìˆ˜ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨')
plt.show()


# - ìœ„ ê·¸ë˜í”„ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤ì‹œí”¼ ê²°ì¸¡ì¹˜ê°€ 100ë§Œê°œ ì´ìƒì¸ ì¹¼ëŸ¼ë“¤ì€ ê²°ì¸¡ì¹˜ê°€ í•´ë‹¹ ì¹¼ëŸ¼ì˜ element ì¤‘ 90% ì´ìƒì„ ì°¨ì§€í•˜ëŠ” ìƒí™©ì´ ë©ë‹ˆë‹¤.
# - ë”°ë¼ì„œ ë³¸ Baseline ì—ì„œëŠ” ì´ ì¹¼ëŸ¼ì€ ì˜ˆì¸¡ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨í•´ ì‚­ì œí•´ì£¼ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

# In[ ]:


# Nullê°’ì´ 100ë§Œê°œ ì´ìƒì¸ ì¹¼ëŸ¼ì€ ì‚­ì œí•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
print('* ê²°ì¸¡ì¹˜ê°€ 100ë§Œê°œ ì´í•˜ì¸ ë³€ìˆ˜ë“¤ :', list(concat.columns[concat.isnull().sum() <= 1000000]))     # ë‚¨ê²¨ì§ˆ ë³€ìˆ˜ë“¤ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
print('* ê²°ì¸¡ì¹˜ê°€ 100ë§Œê°œ ì´ìƒì¸ ë³€ìˆ˜ë“¤ :', list(concat.columns[concat.isnull().sum() >= 1000000]))


# In[ ]:


# ìœ„ì—ì„œ ê²°ì¸¡ì¹˜ê°€ 100ë§Œê°œ ì´í•˜ì¸ ë³€ìˆ˜ë“¤ë§Œ ê³¨ë¼ ìƒˆë¡œìš´ concat_select ê°ì²´ë¡œ ì €ì¥í•´ì¤ë‹ˆë‹¤.
selected = list(concat.columns[concat.isnull().sum() <= 1000000])
concat_select = concat[selected]
len(concat_select.columns)


# In[ ]:


concat_select.isnull().sum()     # ê²°ì¸¡ì¹˜ê°€ 100ë§Œê°œ ì´ˆê³¼ì¸ ì¹¼ëŸ¼ì´ ì œê±°ëœ ëª¨ìŠµì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
# targetë³€ìˆ˜ëŠ” test dataset ê°œìˆ˜ë§Œí¼(9272) ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# In[ ]:


# ì—°ì†í˜• ë³€ìˆ˜ëŠ” ì„ í˜•ë³´ê°„ì„ í•´ì£¼ê³ , ë²”ì£¼í˜•ë³€ìˆ˜ëŠ” ì•Œìˆ˜ì—†ê¸°ì— â€œunknownâ€ì´ë¼ê³  ì„ì˜ë¡œ ë³´ê°„í•´ ì£¼ê² ìŠµë‹ˆë‹¤.
concat_select.info()


# In[ ]:


# ë³¸ë²ˆ, ë¶€ë²ˆì˜ ê²½ìš° floatë¡œ ë˜ì–´ìˆì§€ë§Œ ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì˜ë¯¸ë¥¼ ê°€ì§€ë¯€ë¡œ object(string) í˜•íƒœë¡œ ë°”ê¾¸ì–´ì£¼ê³  ì•„ë˜ ì‘ì—…ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
concat_select['ë³¸ë²ˆ'] = concat_select['ë³¸ë²ˆ'].astype('str')
concat_select['ë¶€ë²ˆ'] = concat_select['ë¶€ë²ˆ'].astype('str')


# In[ ]:


# ë¨¼ì €, ì—°ì†í˜• ë³€ìˆ˜ì™€ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìœ„ infoì— ë”°ë¼ ë¶„ë¦¬í•´ì£¼ê² ìŠµë‹ˆë‹¤.
continuous_columns = []
categorical_columns = []

for column in concat_select.columns:
    if pd.api.types.is_numeric_dtype(concat_select[column]):
        continuous_columns.append(column)
    else:
        categorical_columns.append(column)

print("ì—°ì†í˜• ë³€ìˆ˜:", len(continuous_columns), continuous_columns)
print("ë²”ì£¼í˜• ë³€ìˆ˜:", len(categorical_columns), categorical_columns)


# - ì—°ì†í˜• ë³€ìˆ˜ì™€ ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ì•Œë§ê²Œ ë‚˜ëˆ„ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•´ë³´ê³ , ì´ì œ ê°ê° ë³´ê°„ì„ ì§„í–‰í•©ë‹ˆë‹¤.

# In[ ]:


# ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•œ ë³´ê°„
concat_select[categorical_columns] = concat_select[categorical_columns].fillna('NULL')

# ì—°ì†í˜• ë³€ìˆ˜ì— ëŒ€í•œ ë³´ê°„ (ì„ í˜• ë³´ê°„)
concat_select[continuous_columns] = concat_select[continuous_columns].interpolate(method='linear', axis=0)


# In[ ]:


concat_select.isnull().sum()         # ê²°ì¸¡ì¹˜ê°€ ë³´ê°„ëœ ëª¨ìŠµì„ í™•ì¸í•´ë´…ë‹ˆë‹¤.


# - Baselineì—ì„œëŠ” ë‹¨ìˆœ ê²°ì¸¡ì¹˜ë“¤ì„ ì œê±°í•˜ê±°ë‚˜ ë³´ê°„í–ˆì§€ë§Œ,
# EDAë¥¼ í†µí•´ ë³€ìˆ˜ë“¤ì˜ íë¦„ì„ ì¡°ê¸ˆ ë” íŒŒì•…í•˜ê³  ë” ë‚˜ì€ interpolation ë“±ì˜ ë°©ë²•ì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

# ### 3.2. ì´ìƒì¹˜ ì²˜ë¦¬
# - ì´ë²ˆì—” ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•´ë´…ì‹œë‹¤.
# - ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì€ IQR, Z-score ë“± ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ì‰½ê³  ì§ê´€ì ìœ¼ë¡œ ì ‘ê·¼í•˜ëŠ” ë°©ë²•ì€ ê¸°ë³¸ í†µê³„ì¹˜ë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
# - ë³¸ baselineì—ì„œëŠ” IQRë¥¼ ì´ìš©í•œ ë°©ë²•ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.

# In[ ]:


# ì´ìƒì¹˜ ì œê±° ì´ì „ì˜ shapeì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
print(concat_select.shape)


# In[ ]:


# ëŒ€í‘œì ì¸ ì—°ì†í˜• ë³€ìˆ˜ì¸ â€œì „ìš© ë©´ì â€ ë³€ìˆ˜ ê´€ë ¨í•œ ë¶„í¬ë¥¼ ë¨¼ì € ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
fig = plt.figure(figsize=(7, 3))
sns.boxplot(data = concat_select, x = 'ì „ìš©ë©´ì ', color='lightgreen')
plt.title('ì „ìš©ë©´ì  ë¶„í¬')
plt.xlabel('Area')
plt.show()


# In[ ]:


# ì´ìƒì¹˜ ì œê±° ë°©ë²•ì—ëŠ” IQRì„ ì´ìš©í•˜ê² ìŠµë‹ˆë‹¤.
def remove_outliers_iqr(dt, column_name):
    df = dt.query('is_test == 0')       # train data ë‚´ì— ìˆëŠ” ì´ìƒì¹˜ë§Œ ì œê±°í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
    df_test = dt.query('is_test == 1')

    Q1 = df[column_name].quantile(0.25)
    Q3 = df[column_name].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]

    result = pd.concat([df, df_test])   # test dataì™€ ë‹¤ì‹œ í•©ì³ì£¼ê² ìŠµë‹ˆë‹¤.
    return result



# ìœ„ ë°©ë²•ìœ¼ë¡œ ì „ìš© ë©´ì ì— ëŒ€í•œ ì´ìƒì¹˜ë¥¼ ì œê±°í•´ë³´ê² ìŠµë‹ˆë‹¤.
concat_select = remove_outliers_iqr(concat_select, 'ì „ìš©ë©´ì ')



# ì´ìƒì¹˜ ì œê±° í›„ì˜ shapeì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ì•½ 10ë§Œê°œì˜ ë°ì´í„°ê°€ ì œê±°ëœ ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
print(concat_select.shape)

concat_select['is_test'].value_counts()     # ë˜í•œ, train dataë§Œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.

# ## 4. Feature Engineering
# - ì´ì œ íŒŒìƒë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
# - íŒŒìƒ ë³€ìˆ˜ëŠ” ë„ë©”ì¸ ì§€ì‹ì— ê¸°ë°˜í•´ ì œì‘í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
# - ì£¼íƒì˜ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì´ê¸° ë•Œë¬¸ì— ì•½ê°„ì˜ ë¶€ë™ì‚° ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒìƒ ë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

# ì‹œêµ°êµ¬, ë…„ì›” ë“± ë¶„í• í•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ë“¤ì€ ì„¸ë¶€ì‚¬í•­ ê³ ë ¤ë¥¼ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ ëª¨ë‘ ë¶„í• í•´ ì£¼ê² ìŠµë‹ˆë‹¤.
concat_select['êµ¬'] = concat_select['ì‹œêµ°êµ¬'].map(lambda x : x.split()[1])
concat_select['ë™'] = concat_select['ì‹œêµ°êµ¬'].map(lambda x : x.split()[2])
del concat_select['ì‹œêµ°êµ¬']

concat_select['ê³„ì•½ë…„'] = concat_select['ê³„ì•½ë…„ì›”'].astype('str').map(lambda x : x[:4])
concat_select['ê³„ì•½ì›”'] = concat_select['ê³„ì•½ë…„ì›”'].astype('str').map(lambda x : x[4:])
del concat_select['ê³„ì•½ë…„ì›”']


concat_select.columns

# - ì„œìš¸ì˜ ì§‘ê°’ì€ ê°•ë‚¨, ê°•ë¶ ì—¬ë¶€ì— ë”°ë¼ ì°¨ì´ê°€ ë§ì´ ë‚œë‹¤ëŠ” ì‚¬ì‹¤ì€ ë§ì´ ì•Œë ¤ì§„ ì‚¬ì‹¤ì…ë‹ˆë‹¤.
# - ë”°ë¼ì„œ ê°•ë‚¨/ê°•ë¶ì˜ ì—¬ë¶€ì— ë”°ë¼ íŒŒìƒë³€ìˆ˜ë¥¼ ìƒì„±í•´ì£¼ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

all = list(concat_select['êµ¬'].unique())
gangnam = ['ê°•ì„œêµ¬', 'ì˜ë“±í¬êµ¬', 'ë™ì‘êµ¬', 'ì„œì´ˆêµ¬', 'ê°•ë‚¨êµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬']
gangbuk = [x for x in all if x not in gangnam]

assert len(all) == len(gangnam) + len(gangbuk)       # ì•Œë§ê²Œ ë¶„ë¦¬ë˜ì—ˆëŠ”ì§€ ì²´í¬í•©ë‹ˆë‹¤.

# ê°•ë‚¨ì˜ ì—¬ë¶€ë¥¼ ì²´í¬í•©ë‹ˆë‹¤.
is_gangnam = []
for x in concat_select['êµ¬'].tolist() :
  if x in gangnam :
    is_gangnam.append(1)
  else :
    is_gangnam.append(0)

# íŒŒìƒë³€ìˆ˜ë¥¼ í•˜ë‚˜ ë§Œë¦…ë‹ˆë‹¤.
concat_select['ê°•ë‚¨ì—¬ë¶€'] = is_gangnam

print(concat_select.columns)

len(concat_select.columns)


# - ë˜í•œ ì‹ ì¶•ì¸ì§€, êµ¬ì¶•ì¸ì§€ì˜ ì—¬ë¶€ë„ ì‹¤ê±°ë˜ê°€ì— í° ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# - ë”°ë¼ì„œ ê±´ì¶•ë…„ë„ì— ë”°ë¼ íŒŒìƒë³€ìˆ˜ë¥¼ ì œì‘í•´ì£¼ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

# ê±´ì¶•ë…„ë„ ë¶„í¬ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. íŠ¹íˆ 2005ë…„ì´ Q3ì— í•´ë‹¹í•©ë‹ˆë‹¤.
# 2009ë…„ ì´í›„ì— ì§€ì–´ì§„ ê±´ë¬¼ì€ 10%ì •ë„ ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
concat_select['ê±´ì¶•ë…„ë„'].describe(percentiles = [0.1, 0.25, 0.5, 0.75, 0.8, 0.9])

# ë”°ë¼ì„œ 2009ë…„ ì´í›„ì— ì§€ì–´ì¡Œìœ¼ë©´ ë¹„êµì  ì‹ ì¶•ì´ë¼ê³  íŒë‹¨í•˜ê³ , ì‹ ì¶• ì—¬ë¶€ ë³€ìˆ˜ë¥¼ ì œì‘í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
concat_select['ì‹ ì¶•ì—¬ë¶€'] = concat_select['ê±´ì¶•ë…„ë„'].apply(lambda x: 1 if x >= 2009 else 0)

concat_select.head(1)       # ìµœì¢… ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

concat_select.shape

# - ìœ„ ë‘ ì‚¬í•­ ì™¸ì—ë„ ì™¸ë¶€ ê³µê³µ ë°ì´í„°ë¥¼ ì´ìš©í•˜ê±°ë‚˜, EDAë¥¼ í†µí•´ ë” ë§ì€ ë³€ìˆ˜ë¥¼ ì œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ## 5. Model Training

# - ì´ì œ ìœ„ì—ì„œ ë§Œë“  íŒŒìƒë³€ìˆ˜ë“¤ê³¼ ì •ì œí•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³¸ê²©ì ìœ¼ë¡œ ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ë§ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
# - ëª¨ë¸ë§ì—ëŠ” `sklearn`ì˜ `RandomForest`ë¥¼ ì´ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
# 
# - ì°¸ê³  âœ…
#   - `RandomForest`ëŠ” ë°°ê¹…(Bagging)ì˜ ì¼ì¢…ìœ¼ë¡œ, í•™ìŠµì‹œí‚¤ëŠ” ë°ì´í„° ë¿ ì•„ë‹ˆë¼ íŠ¹ì„±ë³€ìˆ˜(X)ë“¤ë„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•´ íŠ¸ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
#   - ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ Nê°œì˜ Tree ìƒì„±í•˜ê³ , Nê°œì˜ Treeì—ì„œ ë°œìƒí•œ Outputì„ Voting(ë²”ì£¼í˜•, ë¶„ë¥˜ë¬¸ì œ)í•˜ê±°ë‚˜, Average(ì—°ì†í˜•, íšŒê·€ë¬¸ì œ)í•´ ìµœì¢… Output ìƒì„±í•©ë‹ˆë‹¤.
#   - ì´ëŠ” High variance, Low bias ìƒí™©ì—ì„œ ë¶„ì‚°(Variance) ê°ì†Œì— ë„ì›€ì„ ì¤ë‹ˆë‹¤.

# ì´ì œ ë‹¤ì‹œ trainê³¼ test datasetì„ ë¶„í• í•´ì¤ë‹ˆë‹¤. ìœ„ì—ì„œ ì œì‘í•´ ë†“ì•˜ë˜ is_test ì¹¼ëŸ¼ì„ ì´ìš©í•©ë‹ˆë‹¤.
dt_train = concat_select.query('is_test==0')
dt_test = concat_select.query('is_test==1')

# ì´ì œ is_test ì¹¼ëŸ¼ì€ dropí•´ì¤ë‹ˆë‹¤.
dt_train.drop(['is_test'], axis = 1, inplace=True)
dt_test.drop(['is_test'], axis = 1, inplace=True)
print(dt_train.shape, dt_test.shape)

dt_test.head(1)

# dt_testì˜ targetì€ ì¼ë‹¨ 0ìœ¼ë¡œ ì„ì˜ë¡œ ì±„ì›Œì£¼ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
dt_test['target'] = 0


# ### 5.1. ë²”ì£¼í˜• ë³€ìˆ˜ Encoding
# - ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ëª¨ë¸ì— íˆ¬ì…í•˜ë©´, ëª¨ë¸ì´ ì œëŒ€ë¡œ ì‘ë™í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
# - ë”°ë¼ì„œ **ë ˆì´ë¸” ì¸ì½”ë”© ê³¼ì •**ì„ í†µí•´ ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ì„ numericí•˜ê²Œ ë°”ê¾¸ëŠ” ì¸ì½”ë”© ê³¼ì •ì„ ì§„í–‰í•´ì£¼ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

# íŒŒìƒë³€ìˆ˜ ì œì‘ìœ¼ë¡œ ì¶”ê°€ëœ ë³€ìˆ˜ë“¤ì´ ì¡´ì¬í•˜ê¸°ì—, ë‹¤ì‹œí•œë²ˆ ì—°ì†í˜•ê³¼ ë²”ì£¼í˜• ì¹¼ëŸ¼ì„ ë¶„ë¦¬í•´ì£¼ê² ìŠµë‹ˆë‹¤.
continuous_columns_v2 = []
categorical_columns_v2 = []

for column in dt_train.columns:
    if pd.api.types.is_numeric_dtype(dt_train[column]):
        continuous_columns_v2.append(column)
    else:
        categorical_columns_v2.append(column)

print("ì—°ì†í˜• ë³€ìˆ˜:", continuous_columns_v2)
print("ë²”ì£¼í˜• ë³€ìˆ˜:", categorical_columns_v2)

# ì•„ë˜ì—ì„œ ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ ë ˆì´ë¸”ì¸ì½”ë”©ì„ ì§„í–‰í•´ ì£¼ê² ìŠµë‹ˆë‹¤.

# ê° ë³€ìˆ˜ì— ëŒ€í•œ LabelEncoderë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
label_encoders = {}

# Implement Label Encoding
for col in tqdm( categorical_columns_v2 ):
    lbl = LabelEncoder()

    # Label-Encodingì„ fit
    lbl.fit( dt_train[col].astype(str) )
    dt_train[col] = lbl.transform(dt_train[col].astype(str))
    label_encoders[col] = lbl           # ë‚˜ì¤‘ì— í›„ì²˜ë¦¬ë¥¼ ìœ„í•´ ë ˆì´ë¸”ì¸ì½”ë”ë¥¼ ì €ì¥í•´ì£¼ê² ìŠµë‹ˆë‹¤.

    # Test ë°ì´í„°ì—ë§Œ ì¡´ì¬í•˜ëŠ” ìƒˆë¡œ ì¶œí˜„í•œ ë°ì´í„°ë¥¼ ì‹ ê·œ í´ë˜ìŠ¤ë¡œ ì¶”ê°€í•´ì¤ë‹ˆë‹¤.
    for label in np.unique(dt_test[col]):
      if label not in lbl.classes_: # unseen label ë°ì´í„°ì¸ ê²½ìš°
        lbl.classes_ = np.append(lbl.classes_, label) # ë¯¸ì²˜ë¦¬ ì‹œ ValueErrorë°œìƒí•˜ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”!

    dt_test[col] = lbl.transform(dt_test[col].astype(str))

dt_train.head(1)        # ë ˆì´ë¸”ì¸ì½”ë”©ì´ ëœ ëª¨ìŠµì…ë‹ˆë‹¤.


# ### 5.2. Model Training
# - ìœ„ ë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ train í•´ë³´ê² ìŠµë‹ˆë‹¤. ëª¨ë¸ì€ RandomForestë¥¼ ì´ìš©í•˜ê² ìŠµë‹ˆë‹¤.
# - Trainê³¼ Valid datasetì„ ë¶„í• í•˜ëŠ” ê³¼ì •ì—ì„œëŠ” `holdout` ë°©ë²•ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì˜ ê²½ìš°  ëŒ€ëµì ì¸ ì„±ëŠ¥ì„ ë¹ ë¥´ê²Œ í™•ì¸í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ baselineì—ì„œ ì‚¬ìš©í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
#   - ì´ í›„ ì¶”ê°€ì ì¸ edaë¥¼ í†µí•´ì„œ í‰ê°€ì„¸íŠ¸ì™€ ê²½í–¥ì„ ë§ì¶”ê±°ë‚˜ kfoldì™€ ê°™ì€ ë¶„í¬ì— ëŒ€í•œ ê³ ë ¤ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# In[ ]:


assert dt_train.shape[1] == dt_test.shape[1]          # train/test datasetì˜ shapeì´ ê°™ì€ì§€ í™•ì¸í•´ì£¼ê² ìŠµë‹ˆë‹¤.


# In[ ]:
# ì¶”ê°€ëœ ì†ŒìŠ¤!!!!!!!!!!!!!!!!!!!!!!

print(dt_test.columns)
print('ìš”ê¸°ì—ìš” ìš”ê¸°')
# Targetê³¼ ë…ë¦½ë³€ìˆ˜ë“¤ì„ ë¶„ë¦¬í•´ì¤ë‹ˆë‹¤.
# y_train = dt_train['target']
# X_train = dt_train.drop(['target'], axis=1)


X_train = dt_train
# Valid setì„ test.csv ë°ì´í„°ì™€ ë™ì¼í•œ ì¡°ê±´í•˜ì— ë§Œë“¤ê¸° ìœ„í•˜ì—¬ í›ˆë ¨ê¸°ê°„ê³¼ validationê¸°ê°„ì„ ë¶„ë¦¬í•˜ì—¬ Validation setì€ í›ˆë ¨í•˜ì§€ ëª»í•˜ë„ë¡ ì„¤ê³„

# X_train['ê³„ì•½ë…„'].dtypes

print('ì²˜ë¦¬ì „ X_train',X_train.shape)

X_train['ê³„ì•½ë…„'] = X_train['ê³„ì•½ë…„'].astype(str)
X_train['ê³„ì•½ì›”'] = X_train['ê³„ì•½ì›”'].astype(str)

# print("ê³„ì•½ì›”",set(X_train['ê³„ì•½ì›”']))

X_val = X_train[(X_train['ê³„ì•½ë…„'] == '2023')|(X_train['ê³„ì•½ì›”'] == '6')]
X_train = X_train[~((X_train['ê³„ì•½ë…„'] == '2023')|(X_train['ê³„ì•½ì›”'] == '6')) ]

print('ì²˜ë¦¬í›„ X_train',X_train.shape)
print('ì²˜ë¦¬í›„ X_Val',X_val.shape)

X_train['ê³„ì•½ë…„'] = X_train['ê³„ì•½ë…„'].astype(int)
X_train['ê³„ì•½ì›”'] = X_train['ê³„ì•½ì›”'].astype(int)

X_val['ê³„ì•½ë…„'] = X_val['ê³„ì•½ë…„'].astype(int)
X_val['ê³„ì•½ì›”'] = X_val['ê³„ì•½ì›”'].astype(int)

y_train = X_train['target']
X_train = X_train.drop(['target'], axis=1)

y_val = X_val['target']
X_val = X_val.drop(['target'], axis=1)

print('target ë¶„ë¦¬ í›„ X_train',X_train.shape)
print('target ë¶„ë¦¬ í›„ y_train',y_train.shape)
print('target ë¶„ë¦¬ í›„ X_val',X_val.shape)
print('target ë¶„ë¦¬ í›„ y_val',y_val.shape)
print('target ë¶„ë¦¬ í›„ X_val',y_train.squeeze().unique())
print('target ë¶„ë¦¬ í›„ y_val',y_val.squeeze().unique())
# print("í›ˆë ¨ ë°ì´í„° ê³ ìœ  ê°’:", set(y_train))
# print("ê²€ì¦ ë°ì´í„° ê³ ìœ  ê°’:", set(y_val))

# # Hold out splitì„ ì‚¬ìš©í•´ í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ê² ìŠµë‹ˆë‹¤.
# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2023)

# RandomForestRegressorë¥¼ ì´ìš©í•´ íšŒê·€ ëª¨ë¸ì„ ì í•©ì‹œí‚¤ê² ìŠµë‹ˆë‹¤.
model = RandomForestRegressor(n_estimators=5, criterion='squared_error', random_state=1, n_jobs=-1)
model.fit(X_train, y_train)
pred = model.predict(X_val)

# - ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë„ ë°ì´í„°ì— ë§ê²Œ ì§€ì •í•´ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ì— ë§ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ê²ƒë„ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# íšŒê·€ ê´€ë ¨ metricì„ í†µí•´ train/validì˜ ëª¨ë¸ ì í•© ê²°ê³¼ë¥¼ ê´€ì°°í•©ë‹ˆë‹¤.
print(f'RMSE test: {np.sqrt(metrics.mean_squared_error(y_val, pred))}')

# ë³€ìˆ˜ ì¤‘ìš”ë„ë„ í™•ì¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

# ìœ„ feature importanceë¥¼ ì‹œê°í™”í•´ë´…ë‹ˆë‹¤.
importances = pd.Series(model.feature_importances_, index=list(X_train.columns))
importances = importances.sort_values(ascending=False)

plt.figure(figsize=(10,8))
plt.title("Feature Importances")
sns.barplot(x=importances, y=importances.index)
plt.show()

# í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤. Pickle ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ê² ìŠµë‹ˆë‹¤.
with open('saved_model.pkl', 'wb') as f:
    pickle.dump(model, f)

# ### 5.3. Feature selection
# ì–´ë–¤ ë³€ìˆ˜ê°€ ìœ ìš©í•œ ë³€ìˆ˜ì¸ì§€ í™•ì¸í•´ë³´ê¸° ìœ„í•´ permutation selectionì„ ì´ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.

# Permutation importance ë°©ë²•ì„ ë³€ìˆ˜ ì„ íƒì— ì´ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.
perm = PermutationImportance(model,        # ìœ„ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•˜ê² ìŠµë‹ˆë‹¤.
                             scoring = "neg_mean_squared_error",        # í‰ê°€ ì§€í‘œë¡œëŠ” íšŒê·€ë¬¸ì œì´ê¸°ì— negative rmseë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (neg_mean_squared_error : ìŒì˜ í‰ê·  ì œê³± ì˜¤ì°¨)
                             random_state = 42,
                             n_iter=3).fit(X_val, y_val)
eli5.show_weights(perm, feature_names = X_val.columns.tolist())    # valid dataì— ëŒ€í•´ ì í•©ì‹œí‚µë‹ˆë‹¤.

# - ë¶„ì„ ê²°ê³¼ "ê³„ì•½ë…„", "ì „ìš©ë©´ì " ë³€ìˆ˜ê°€ ìœ ì˜í•œ ë³€ìˆ˜ë¡œ ë³´ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë³€ìˆ˜ë¡œ ë˜ ë‹¤ë¥¸ íŒŒìƒë³€ìˆ˜ë¥¼ ë” ìƒì„±í•´ë³´ê±°ë‚˜, ì¤‘ìš”ë„ê°€ ë‚®ì•„ë³´ì´ëŠ” ë³€ìˆ˜ë¥¼ ì œê±°í•´ ì°¨ì›ì˜ ì €ì£¼ë¥¼ ë§‰ì•„ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

# ### 5.4. Valid prediction ë¶„ì„

# - ì˜ˆì¸¡ê°’ì„ ë¶„ì„í•´ë³´ê¸° ìœ„í•´ valid predictionì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

# Validation datasetì— targetê³¼ pred ê°’ì„ ì±„ì›Œì£¼ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
X_val['target'] = y_val
X_val['pred'] = pred

# Squared_errorë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.
def calculate_se(target, pred):
    squared_errors = (target - pred) ** 2
    return squared_errors

# RMSE ê³„ì‚°
squared_errors = calculate_se(X_val['target'], X_val['pred'])
X_val['error'] = squared_errors

# Errorê°€ í° ìˆœì„œëŒ€ë¡œ sorting í•´ ë³´ê² ìŠµë‹ˆë‹¤.
X_val_sort = X_val.sort_values(by='error', ascending=False)       # ë‚´ë¦¼ì°¨ìˆœ sorting

X_val_sort.head()

# - ì˜ˆì¸¡ì„ ì˜ í•˜ì§€ ëª»í•œ top 100ê°œì˜ ë°ì´í„°ì™€ ì˜ˆì¸¡ì„ ì˜í•œ top 100ê°œì˜ ë°ì´í„°ë¥¼ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤.

X_val_sort_top100 = X_val.sort_values(by='error', ascending=False).head(100)        # ì˜ˆì¸¡ì„ ì˜ í•˜ì§€ëª»í•œ top 100ê°œì˜ data
X_val_sort_tail100 = X_val.sort_values(by='error', ascending=False).tail(100)       # ì˜ˆì¸¡ì„ ì˜í•œ top 100ê°œì˜ data

# í•´ì„ì„ ìœ„í•´ ë ˆì´ë¸”ì¸ì½”ë”© ëœ ë³€ìˆ˜ë¥¼ ë³µì›í•´ì¤ë‹ˆë‹¤.
error_top100 = X_val_sort_top100.copy()
for column in categorical_columns_v2 :     # ì•ì„œ ë ˆì´ë¸” ì¸ì½”ë”©ì—ì„œ ì •ì˜í–ˆë˜ categorical_columns_v2 ë²”ì£¼í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    error_top100[column] = label_encoders[column].inverse_transform(X_val_sort_top100[column])

best_top100 = X_val_sort_tail100.copy()
for column in categorical_columns_v2 :     # ì•ì„œ ë ˆì´ë¸” ì¸ì½”ë”©ì—ì„œ ì •ì˜í–ˆë˜ categorical_columns_v2 ë²”ì£¼í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    best_top100[column] = label_encoders[column].inverse_transform(X_val_sort_tail100[column])

display(error_top100.head(1))
display(best_top100.head(1))

# - ì´ì œ ë¶„í¬ë¥¼ ë¹„êµí•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

sns.boxplot(data = error_top100, x='target')
plt.title('The worst top100 predictionì˜ target ë¶„í¬')
plt.show()

sns.boxplot(data = best_top100, x='target', color='orange')
plt.title('The best top100 predictionì˜ target ë¶„í¬')
plt.show()

# - Taget ë¶„í¬ë¥¼ ë³´ë‹ˆ ì¢‹ì€ ì˜ˆì¸¡ì„ ë³´ì¸ top 100ê°œì˜ dataë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ë‚˜ìœ ì˜ˆì¸¡ì„ ë³´ì¸ top 100 datasetë“¤ì´ ë†’ì€ ê°€ê²©ì„ ë³´ì˜€ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•œ ëª¨ë¸ë§ ë° ì²˜ë¦¬ê°€ í•„ìš”í•´ë³´ì…ë‹ˆë‹¤.

sns.histplot(data = error_top100, x='ì „ìš©ë©´ì ', alpha=0.5)
sns.histplot(data = best_top100, x='ì „ìš©ë©´ì ', color='orange', alpha=0.5)
plt.title('ì „ìš©ë©´ì  ë¶„í¬ ë¹„êµ')
plt.show()

# - ì „ìš©ë©´ì  ë˜í•œ ë‚˜ìœ ì˜ˆì¸¡ì„ ë³´ì¸ ì§‘ë“¤ì´ ë” ë„“ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ## 6. Inference

dt_test.head(2)      # test datasetì— ëŒ€í•œ inferenceë¥¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.

# ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
with open('saved_model.pkl', 'rb') as f:
    model = pickle.load(f)

X_test = dt_test.drop(['target'], axis=1)

# Test datasetì— ëŒ€í•œ inferenceë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
real_test_pred = model.predict(X_test)

real_test_pred          # ì˜ˆì¸¡ê°’ë“¤ì´ ì¶œë ¥ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ## 7. Output File Save

# ì•ì„œ ì˜ˆì¸¡í•œ ì˜ˆì¸¡ê°’ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤.
preds_df = pd.DataFrame(real_test_pred.astype(int), columns=["target"])
preds_df.to_csv('output.csv', index=False)